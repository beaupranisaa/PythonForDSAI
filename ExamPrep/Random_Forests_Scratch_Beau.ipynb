{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "So far, it seems bagging works well.  However, the $B$ bootstrapped dataset are correlated, thus the power of variance reduction is diminished.  How do we further de-correlate these $B$ trees?\n",
    "\n",
    "A **random forest** is constructed by bagging, but for each split in each tree, only a random subset of $q \\leq n$ features are considered as splitting variables.\n",
    "\n",
    "Rule of thumb: $q = \\sqrt{n}$ for classification trees and $q = \\frac{n}{3}$ for regression trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,shuffle=True,random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Out of bag score for each tree ======\n",
      "Tree 0 1.0\n",
      "Tree 1 0.9523809523809523\n",
      "Tree 2 0.9523809523809523\n",
      "Tree 3 1.0\n",
      "Tree 4 0.9523809523809523\n",
      "====== Average out of bag score =======\n",
      "0.9714285714285713\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random, math\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy import stats\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, B, bootstrap_ratio, with_no_replacement = True):\n",
    "        self.B = B\n",
    "        self.bootstrap_ratio = bootstrap_ratio\n",
    "        self.with_no_replacement = with_no_replacement\n",
    "        self.tree_params = {'max_depth': 2, 'max_features': 'sqrt'}\n",
    "        self.models = [DecisionTreeClassifier(**self.tree_params) for _ in range(B)]\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        m,n = X.shape\n",
    "        \n",
    "        sample_size = int(self.bootstrap_ratio*len(X))\n",
    "        \n",
    "        xsamples = np.zeros((self.B,sample_size,n))\n",
    "        ysamples = np.zeros((self.B,sample_size))\n",
    "        \n",
    "        xsamples_oob = []\n",
    "        ysamples_oob = []\n",
    "        \n",
    "        for i in range(self.B):\n",
    "            oob_idx = []\n",
    "            idxes = []\n",
    "            for j in range(sample_size):\n",
    "                idx = random.randrange(m)\n",
    "                if (self.with_no_replacement):\n",
    "                    while idx in idxes:\n",
    "                        idx = random.randrange(m)\n",
    "                idxes.append(idx)\n",
    "                oob_idx.append(idx)\n",
    "                xsamples[i,j,:] = X[idx]\n",
    "                ysamples[i,j] = y[idx]\n",
    "            mask = np.zeros((m),dtype=bool)\n",
    "            mask[oob_idx] = True\n",
    "            xsamples_oob.append(X[~mask])\n",
    "            ysamples_oob.append(y[~mask])\n",
    "            \n",
    "        oob_score = 0\n",
    "        print(\"====== Out of bag score for each tree ======\")\n",
    "        for i, model in enumerate(self.models):\n",
    "            _X = xsamples[i]\n",
    "            _y = ysamples[i]\n",
    "            model.fit(_X,_y)\n",
    "            \n",
    "            _X_test = np.asarray(xsamples_oob[i])\n",
    "            _y_test = np.asarray(ysamples_oob[i])\n",
    "            yhat = model.predict(_X_test) #sklearn\n",
    "            oob_score += accuracy_score(_y_test,yhat)\n",
    "            print(f\"Tree {i}\", accuracy_score(_y_test,yhat))\n",
    "        self.avg_oob_score = oob_score/len(self.models)\n",
    "        print(\"====== Average out of bag score =======\")\n",
    "        print(self.avg_oob_score)\n",
    "        \n",
    "    def predict(self,X):\n",
    "        #make prediction and return the probabilities\n",
    "        prediction = np.zeros((self.B,X.shape[0]))\n",
    "        for i, model in enumerate(self.models):\n",
    "            yhat = model.predict(X) #sklearn\n",
    "            prediction[i,:] = yhat\n",
    "        return stats.mode(prediction)[0][0]\n",
    "\n",
    "    \n",
    "model = RandomForest(B=5,bootstrap_ratio = 0.8)\n",
    "model.fit(X_train,y_train)\n",
    "yhat = model.predict(X_test) #our predict\n",
    "print(classification_report(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===Task===\n",
    "\n",
    "<strong>Your work: Let's modify the above scratch code to</strong>\n",
    "    <ol>\n",
    "        <li>To introduce random features during each split.  You may want to use scratch version of DecisionTree you made in previous class so you can implement how the split is done</li>\n",
    "        <li>Change the class into <code>RandomForest</code>.  It should have at least two methods, <code>fit(X_train, y_train)</code>, and <code>predict(X_test)</code></li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 4, 'n_estimators': 100}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#this is the same as RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"n_estimators\": [10, 50, 100], # no. of trees\n",
    "              \"criterion\": [\"gini\", \"entropy\"], \n",
    "              \"max_depth\": np.arange(1, 10)}\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "grid = GridSearchCV(model, param_grid)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "model = grid.best_estimator_\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use Random Forests\n",
    "\n",
    "Advantages of Random Forest:\n",
    "\n",
    "- Voting helps overcome overfitting\n",
    "- Because bagging and pasting support parallel computing (e.g., using <code>n_jobs</code>), they are very popular methods.\n",
    "- Random forest can solve both type of problems that is classification and regression and does a decent estimation at both fronts.\n",
    "- The power to handle large data sets with higher dimensionality. It can handle thousands of input variables and identity most significant variables so it is considered as one of the dimensionality reduction method. Further, the model outputs importance of variable, which can be a very handy feature.  Sklearn implements <code>feature_importances_</code> in <code>RandomForestClassifier</code> which helps you understand which feature is useful for classification in Random Forest\n",
    "- It has an effective method for estimating missing data and maintains accuracy when large proportion of the data are missing (I did not really touch this, but I recommend you to check it out)\n",
    "- It has methods for balancing errors in data sets where classes are imbalanced.\n",
    "- The capability of the above can be extended to unlabeled data, leading to unsupervised clustering,data views and outlier detection.\n",
    "- Just like other ensemble, it works well with structured/tabular data.  Indeed, XGBoost (another ensemble method) is among the best classifier for structured/tabular data and often used for Kaggle competition.  But if we are working with image, sound, brain signal, deep learning remains the way to go.\n",
    "- Unlike Decision Trees, multiple trees can give out probability\n",
    "- Out of bag evaluation is handy\n",
    "\n",
    "Disadvantages of Random Forest:\n",
    "\n",
    "- It surely does a good job at classification but not as for regression problem as it does not gives precise continuous nature prediction. In case of regression, it doesn't predict beyond the range in the training data, and that they may over fit data sets that are particularly noisy.\n",
    "- Random forest can feel like a black box approach for a statistical modelers we have very little control on what the model does. You can at best try different parameters and random seeds.\n",
    "- At one point, more samples will not improve the accuracy, unlike deep neural network\n",
    "- It fails when there are rare outcomes or rare predictors, as the algorithm is based on bootstrap sampling. This makes it non-ideal if you're working with rare personality traits, high segmented customer behavior, or rare variants in genomics research.\n",
    "\n",
    "In conclusion, if you are working with structured/tabular data, and would like high accuracy but does not care much about interpretability (just like most Kaggle competition does), you may want to use ensemble methods (including Random Forests and the like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
