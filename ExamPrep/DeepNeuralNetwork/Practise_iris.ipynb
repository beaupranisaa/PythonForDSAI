{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### NO standardddddizeeee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepNeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralnet.second_version import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_boston\n",
    "# import copy\n",
    "# X,y = load_boston(return_X_y = True)\n",
    "# print('shapes of X, y')\n",
    "# print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes of X, y\n",
      "(442, 10) (442,)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.datasets import load_diabetes\n",
    "# import copy\n",
    "# X,y = load_diabetes(return_X_y = True)\n",
    "# print('shapes of X, y')\n",
    "# print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_breast_cancer\n",
    "# import copy\n",
    "# X,y = load_breast_cancer(return_X_y = True)\n",
    "# print('shapes of X, y')\n",
    "# print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_digits\n",
    "# import copy\n",
    "# X,y = load_digits(return_X_y = True)\n",
    "# print('shapes of X, y')\n",
    "# print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes of X, y\n",
      "(178, 13) (178,)\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import copy\n",
    "X,y = load_wine(return_X_y = True)\n",
    "print('shapes of X, y')\n",
    "print(X.shape,y.shape)\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes of X_train, y_train\n",
      "(124, 13) (124, 1)\n",
      "shapes of X_test, y_test\n",
      "(54, 13) (54, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                test_size=0.3, shuffle=True, random_state=42)\n",
    "\n",
    "#since our train function assumes y to be shape of (n, 1)\n",
    "y_train, y_test = y_train.reshape(-1, 1), y_test.reshape(-1, 1)\n",
    "\n",
    "# double check the shapes\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "# assert len(X_train)  == len(y_train)\n",
    "assert X_test.shape[0] == y_test.shape[0]\n",
    "# assert len(X_test) == len(y_test)\n",
    "\n",
    "print('shapes of X_train, y_train')\n",
    "print(X_train.shape,y_train.shape)\n",
    "print('shapes of X_test, y_test')\n",
    "print(X_test.shape,y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Standardization/Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 One Hot (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124, 3) (54, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "onehot = preprocessing.OneHotEncoder()\n",
    "\n",
    "#sklearn expects a 2D array thus we have to reshape to (-1, 1)\n",
    "y_train_encode = onehot.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test_encode = onehot.fit_transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "print(y_train_encode.shape, y_test_encode.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Accuracy Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def calc_accuracy_linear(model, X_test, y_test):    \n",
    "    #getting the accuracy score with testing data\n",
    "    preds = model.forward(X_test)\n",
    "    print(\"MSE: \", mean_squared_error(y_test, preds))\n",
    "    print(\"r2 score: \", r2_score(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.1 binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def calc_accuracy_binary_classification(model, X_test, y_test):    \n",
    "    #getting the accuracy score with testing data\n",
    "    preds = model.forward(X_test)\n",
    "    preds[preds>0.5] = 1\n",
    "    preds[preds<0.5] = 0\n",
    "    print(\"Accuracy: \", accuracy_score(y_test, preds))\n",
    "    print(\"Classification Report: \", classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.2 multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a simple accuracy function\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def calc_accuracy_multinomial_classification(model, X_test, y_test):    \n",
    "    #getting the accuracy score with testing data\n",
    "    preds = model.forward(X_test)\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    print(\"Accuracy: \", accuracy_score(y_test, preds))\n",
    "    print(\"Classification Report: \", classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Add more fancy stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Operation):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def _output(self, inference: bool) -> ndarray: \n",
    "        return np.maximum(0,self.input_)\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        self.output[self.output <= 0] = 0\n",
    "        self.output[self.output > 0] = 1\n",
    "        return output_grad * self.output \n",
    "\n",
    "class LeakyRelu(Operation):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def _output(self, inference: bool) -> ndarray: \n",
    "        return np.maximum(0.2*self.input_,self.input_)\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        dz = np.ones_like(self.output)\n",
    "        dz[self.output<0] = 0.2\n",
    "        return output_grad * dz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy(Loss):\n",
    "    def __init__(self, eps: float=1e-9):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def _output(self) -> float:\n",
    "\n",
    "        # clipping the softmax output to prevent numeric instability\n",
    "        #numpy.clip(a, a_min, a_max, out=None, **kwargs)\n",
    "        #To prevent extremely large loss values that could lead to numeric instability, \n",
    "        #we’ll clip the output of the softmax function to be no less than 10–7 and no greater than 10^7\n",
    "        self.prediction = np.clip(self.prediction, self.eps, 1 - self.eps)\n",
    "\n",
    "        # actual loss computation\n",
    "        binary_cross_entropy_loss = (\n",
    "            -1.0 * self.target * np.log(self.prediction) - \\\n",
    "                (1.0 - self.target) * np.log(1 - self.prediction)\n",
    "        )\n",
    "        \n",
    "        #return average loss\n",
    "        return np.sum(binary_cross_entropy_loss) / self.prediction.shape[0]\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        #return average grad\n",
    "        return (self.prediction - self.target) / self.prediction.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BinaryCrossEntropy(Loss):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def _output(self) -> float:\n",
    "#         return np.sum(-((1 - self.target) * np.log(1 - self.prediction) + self.target * np.log(self.prediction)))\n",
    "\n",
    "#     def _input_grad(self) -> ndarray:\n",
    "#         return (self.prediction - self.target) / self.prediction.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code: WATCH OUT FOR THE NUMBER OF NEURONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model: Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NeuralNetwork(\n",
    "#     layers=[Dense(neurons=10, \n",
    "#                   activation=Sigmoid(),\n",
    "#                   weight_init=\"glorot\",\n",
    "#                   dropout=0.8),\n",
    "#             Dense(neurons=5, \n",
    "#                   activation=Sigmoid(),\n",
    "#                   weight_init=\"glorot\",\n",
    "#                   dropout=0.8),\n",
    "#             Dense(neurons=1,  ##### Neurons = 1\n",
    "#                   activation=Linear(),  ## END WITH LINEAR\n",
    "#                   weight_init=\"glorot\")],\n",
    "#             loss = MeanSquaredError(), \n",
    "# seed=20200720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 0.640\n",
      "Validation loss after 20 epochs is 0.604\n",
      "Validation loss after 30 epochs is 0.603\n",
      "Validation loss after 40 epochs is 0.602\n",
      "Validation loss after 50 epochs is 0.602\n",
      "Validation loss after 60 epochs is 0.602\n",
      "Loss increased after epoch 70, final loss was 0.602, using the model from epoch 60\n",
      "========= Accuracy ========\n",
      "MSE:  0.5945580052737712\n",
      "r2 score:  0.013243515436358932\n"
     ]
    }
   ],
   "source": [
    "# trainer = Trainer(model,SGD(lr=0.01) )\n",
    "# #SGDMomentum(lr=0.2, momentum=0.9,final_lr=0.05, decay_type='exponential'))\n",
    "# trainer.fit(X_train, y_train, X_test, y_test,\n",
    "#             epochs = 500,\n",
    "#             eval_every = 10,\n",
    "#             seed=20200720,\n",
    "#             batch_size=60)\n",
    "\n",
    "# print('========= Accuracy ========')\n",
    "# calc_accuracy_linear(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model: Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NeuralNetwork(\n",
    "#     layers=[Dense(neurons=20, \n",
    "#                   activation=Tanh(),\n",
    "#                   weight_init=\"glorot\",\n",
    "#                   dropout=0.8),\n",
    "#             Dense(neurons=5, \n",
    "#                   activation=Tanh(),\n",
    "#                   weight_init=\"glorot\",\n",
    "#                   dropout=0.8),\n",
    "#             Dense(neurons=1,  ##### Neurons = 1\n",
    "#                   activation=Sigmoid(),  ##### SIGMOID\n",
    "#                   weight_init=\"glorot\")],\n",
    "#             loss = MeanSquaredError(), \n",
    "# seed=20200720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NeuralNetwork(\n",
    "#     layers=[Dense(neurons=20, \n",
    "#                   activation=Sigmoid(),\n",
    "#                   weight_init=\"glorot\",\n",
    "#                   dropout=0.8),\n",
    "#             Dense(neurons=5, \n",
    "#                   activation=Sigmoid(),\n",
    "#                   weight_init=\"glorot\",\n",
    "#                   dropout=0.8),\n",
    "#             Dense(neurons=1, \n",
    "#                   activation=Sigmoid(), ##### SIGMOID\n",
    "#                   weight_init=\"glorot\")],\n",
    "#             loss = BinaryCrossEntropy(), \n",
    "# seed=20200720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 0.651\n",
      "Validation loss after 20 epochs is 0.613\n",
      "Validation loss after 30 epochs is 0.451\n",
      "Validation loss after 40 epochs is 0.296\n",
      "Validation loss after 50 epochs is 0.226\n",
      "========= Accuracy ========\n",
      "Accuracy:  0.9064327485380117\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.87        63\n",
      "           1       0.90      0.95      0.93       108\n",
      "\n",
      "    accuracy                           0.91       171\n",
      "   macro avg       0.91      0.89      0.90       171\n",
      "weighted avg       0.91      0.91      0.91       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trainer = Trainer(model,SGDMomentum(lr=0.2, momentum=0.9,\n",
    "#                                     final_lr=0.05, decay_type='exponential'))\n",
    "# trainer.fit(X_train, y_train, X_test, y_test,\n",
    "#             epochs = 50,\n",
    "#             eval_every = 10,\n",
    "#             seed=20200720,\n",
    "#             batch_size=50)\n",
    "\n",
    "# print('========= Accuracy ========')\n",
    "# calc_accuracy_binary_classification(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model: Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=4, \n",
    "                  activation=Tanh(),\n",
    "                  weight_init=\"glorot\",\n",
    "                  dropout=0.8),\n",
    "            Dense(neurons=3, \n",
    "                  activation=Tanh(),\n",
    "                  weight_init=\"glorot\",\n",
    "                  dropout=0.8),\n",
    "            Dense(neurons=3, \n",
    "                  activation=Linear(),\n",
    "                  weight_init=\"glorot\")],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20200720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 0.370\n",
      "Validation loss after 20 epochs is 0.248\n",
      "Loss increased after epoch 30, final loss was 0.248, using the model from epoch 20\n",
      "========= Accuracy ========\n",
      "Accuracy:  0.8703703703703703\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92        19\n",
      "           1       0.94      0.81      0.87        21\n",
      "           2       0.75      0.86      0.80        14\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.86      0.87      0.86        54\n",
      "weighted avg       0.88      0.87      0.87        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, SGDMomentum(lr=0.2, momentum=0.9,\n",
    "                                    final_lr=0.05, decay_type='exponential'))\n",
    "trainer.fit(X_train, y_train_encode, X_test, y_test_encode,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20200720,\n",
    "            batch_size=20)\n",
    "\n",
    "print('========= Accuracy ========')\n",
    "calc_accuracy_multinomial_classification(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
