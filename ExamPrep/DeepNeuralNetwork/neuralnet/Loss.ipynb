{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss\n",
    "class Loss(object):\n",
    "   \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, prediction: ndarray, target: ndarray) -> float:\n",
    "        assert prediction.shape == target.shape\n",
    "\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        \n",
    "        #self._output will hold the loss function\n",
    "        loss_value = self._output()\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def backward(self) -> ndarray:\n",
    "\n",
    "        self.input_grad = self._input_grad()\n",
    "\n",
    "        assert self.prediction.shape == self.input_grad.shape\n",
    "\n",
    "        #input_grad will hold the gradient of the loss function\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class MeanSquaredError(Loss):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        loss = (\n",
    "            np.sum(np.power(self.prediction - self.target, 2)) / \n",
    "            self.prediction.shape[0]\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        return 2.0 * (self.prediction - self.target) / self.prediction.shape[0]\n",
    "\n",
    "class SoftmaxCrossEntropy(Loss):\n",
    "    def __init__(self, eps: float=1e-9):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        \n",
    "        # applying the softmax function to each row (observation)\n",
    "        softmax_preds = self.softmax(self.prediction, axis=1)\n",
    "\n",
    "        # clipping the softmax output to prevent numeric instability\n",
    "        #numpy.clip(a, a_min, a_max, out=None, **kwargs)\n",
    "        #To prevent extremely large loss values that could lead to numeric instability, \n",
    "        #we’ll clip the output of the softmax function to be no less than 10–7 and no greater than 10^7\n",
    "        self.softmax_preds = np.clip(softmax_preds, self.eps, 1 - self.eps)\n",
    "\n",
    "        # actual loss computation\n",
    "        softmax_cross_entropy_loss = (\n",
    "            -1.0 * self.target * np.log(self.softmax_preds) - \\\n",
    "                (1.0 - self.target) * np.log(1 - self.softmax_preds)\n",
    "        )\n",
    "        \n",
    "        #return average loss\n",
    "        return np.sum(softmax_cross_entropy_loss) / self.prediction.shape[0]\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        #return average grad\n",
    "        return (self.softmax_preds - self.target) / self.prediction.shape[0]\n",
    "\n",
    "    def softmax(self, x, axis=None):\n",
    "        #keepdims so that this number can be broadcasted and divided\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=axis, keepdims=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
