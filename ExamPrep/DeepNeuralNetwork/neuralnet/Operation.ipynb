{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "#Operations\n",
    "class Operation(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self,\n",
    "                input_: ndarray,\n",
    "                inference: bool=False) -> ndarray:  #<----inference\n",
    "\n",
    "        self.input_ = input_\n",
    "\n",
    "        self.output = self._output(inference) #<----inference\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "\n",
    "        #make sure output and output_grad has same shape\n",
    "        assert self.output.shape == output_grad.shape\n",
    "\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "\n",
    "        #input grad must have same shape as input\n",
    "        assert self.input_.shape == self.input_grad.shape\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self, inference: bool) -> ndarray:  #<----inference\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class ParamOperation(Operation):\n",
    "    def __init__(self, param: ndarray):\n",
    "        super().__init__()  #inherit from parent if any\n",
    "        self.param = param  #this will be used in _output\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        \n",
    "        #make sure output and output_grad has same shape\n",
    "        assert self.output.shape == output_grad.shape\n",
    "\n",
    "        #perform gradients for both input and param\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "\n",
    "        assert self.input_.shape == self.input_grad.shape\n",
    "        assert self.param.shape == self.param_grad.shape\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        raise NotImplementedError()  \n",
    "\n",
    "class WeightMultiply(ParamOperation):\n",
    "\n",
    "    def __init__(self, W: ndarray):\n",
    "        #initialize Operation with self.param = W\n",
    "        super().__init__(W)\n",
    "\n",
    "    def _output(self, inference: bool) -> ndarray: #<----inference\n",
    "        return self.input_ @ self.param\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return output_grad @ self.param.T  #same as last class\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray)  -> ndarray:\n",
    "        return self.input_.T @ output_grad  #same as last class\n",
    "\n",
    "class BiasAdd(ParamOperation):\n",
    "    def __init__(self, B: ndarray):\n",
    "        #initialize Operation with self.param = B.\n",
    "        assert B.shape[0] == 1  #make sure it's only B\n",
    "        super().__init__(B)\n",
    "\n",
    "    def _output(self, inference: bool) -> ndarray: #<----inference\n",
    "        return self.input_ + self.param\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return np.ones_like(self.input_) * output_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        param_grad = np.ones_like(self.param) * output_grad\n",
    "        return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1])\n",
    "\n",
    "\n",
    "class Linear(Operation):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self, inference: bool) -> ndarray:   #<----inference\n",
    "        return self.input_\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return output_grad\n",
    "\n",
    "\n",
    "class Sigmoid(Operation):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self, inference: bool) -> ndarray:   #<----inference\n",
    "        return 1.0/(1.0+np.exp(-1.0 * self.input_))\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        sigmoid_backward = self.output * (1.0 - self.output)\n",
    "        input_grad = sigmoid_backward * output_grad\n",
    "        return input_grad\n",
    "\n",
    "\n",
    "class Tanh(Operation):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self, inference: bool) -> ndarray:  #<----inference\n",
    "        return np.tanh(self.input_)\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return output_grad * (1 - self.output * self.output)\n",
    "\n",
    "\n",
    "#we have to define Dropout again, so it refers to the new Operation class\n",
    "class Dropout(Operation):\n",
    "\n",
    "    def __init__(self,\n",
    "                 keep_prob: float = 0.8):\n",
    "        super().__init__()\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "    def _output(self, inference: bool) -> ndarray: \n",
    "        if inference:\n",
    "            return self.input_ * self.keep_prob  #multiply input by probability\n",
    "        else:\n",
    "            #binomial will give us list of 0 and 1s with 1s of probability equal to keep_prob\n",
    "            self.mask = np.random.binomial(1, self.keep_prob,\n",
    "                                           size=self.input_.shape)  \n",
    "            return self.input_ * self.mask\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        #since gradient of 0 is nothing, thus the input_grad is simply whatever output_grad multiply with self.mask\n",
    "        return output_grad * self.mask\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
