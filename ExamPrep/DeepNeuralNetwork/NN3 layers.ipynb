{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved model:NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from typing import Callable\n",
    "from typing import Dict\n",
    "from typing import Callable, Dict, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x:ndarray) -> ndarray:\n",
    "    return 1/(1+np.exp(-1.0*x))\n",
    "def leaky_relu(x:ndarray) -> ndarray:\n",
    "    return np.maximum(0.2* x,x)\n",
    "def dev_sigmoid(x:ndarray) -> ndarray:\n",
    "    sigm = sigmoid(x)\n",
    "    return sigm*(1-sigm)\n",
    "def dev_leaky_relu(x:ndarray) -> ndarray:\n",
    "    dx = np.ones_like(x)\n",
    "    dx[x < 0] = 0.2\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(input_size:int, \n",
    "                hidden_size1:int, hidden_size2: int) -> Dict[str,ndarray]:\n",
    "    weights: Dict[str,ndarray] = {}\n",
    "    weights['W1'] = np.random.randn(input_size,hidden_size1)\n",
    "    weights['B1'] = np.random.randn(1,hidden_size1)\n",
    "    weights['W2'] = np.random.randn(hidden_size1,hidden_size2)\n",
    "    weights['B2'] = np.random.randn(1,hidden_size2)\n",
    "    weights['W3'] = np.random.randn(hidden_size2,1)\n",
    "    weights['B3'] = np.random.randn(1,1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X: ndarray, y: ndarray,\n",
    "            weights: Dict[str, ndarray]\n",
    "           )-> Tuple[float, Dict[str, ndarray]]:\n",
    "    M1 = X @ weights['W1']\n",
    "    N1 = M1 + weights['B1']\n",
    "    O1 = leaky_relu(N1)\n",
    "    M2 = O1 @ weights['W2']\n",
    "    N2 = M2 + weights['B2']\n",
    "    O2 = sigmoid(N2)\n",
    "    M3 = O2 @ weights['W3']\n",
    "    P = M3 + weights['B3']\n",
    "    loss = np.mean(np.power(y-P,2))\n",
    "#     print(\"=================1===================\")\n",
    "#     print(weights['W1'].shape,M1.shape,N1.shape,O1.shape)\n",
    "#     print(\"=================2===================\")\n",
    "#     print(weights['W2'].shape,M2.shape,N2.shape,O2.shape)\n",
    "#     print(\"=================3===================\")\n",
    "#     print(weights['W3'].shape,M3.shape,P.shape,loss.shape)\n",
    "    \n",
    "    forward_info: Dict[str,ndarray] = {}\n",
    "    forward_info['X'] = X\n",
    "    forward_info['M1'] = M1\n",
    "    forward_info['N1'] = N1\n",
    "    forward_info['O1'] = O1\n",
    "    forward_info['M2'] = M2\n",
    "    forward_info['N2'] = N2\n",
    "    forward_info['O2'] = O2\n",
    "    forward_info['M3'] = M3\n",
    "    forward_info['P'] = P\n",
    "    forward_info['y'] = y\n",
    "    forward_info['loss'] = loss\n",
    "    return forward_info, loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_gradients(forward_info: Dict[str,ndarray], \n",
    "                   weights:Dict[str,ndarray]) -> Dict[str,ndarray]:\n",
    "    X = forward_info['X']\n",
    "    M1 = forward_info['M1']\n",
    "    N1 = forward_info['N1']\n",
    "    O1 = forward_info['O1']\n",
    "    M2 = forward_info['M2']\n",
    "    N2 = forward_info['N2']\n",
    "    O2 = forward_info['O2']\n",
    "    M3 = forward_info['M3']\n",
    "    P = forward_info['P']\n",
    "    y = forward_info['y']\n",
    "    loss = forward_info['loss']\n",
    "    \n",
    "    W1 = weights['W1']\n",
    "    W2 = weights['W2']\n",
    "    W3 = weights['W3']\n",
    "    B1 = weights['B1']\n",
    "    B2 = weights['B2']\n",
    "    B3 = weights['B3']\n",
    "    \n",
    "    dLdP = P - y\n",
    "    dPdM3 = np.ones_like(M3)\n",
    "    dPdB3 = np.ones_like(B3)\n",
    "    dM3dW3 = O2.T\n",
    "#            (10, 23)     (23, 1)    (23, 1)\n",
    "#     print(dM3dW3.shape, dLdP.shape, dPdM3.shape)\n",
    "    dLdW3 = dM3dW3 @ (dLdP * dPdM3)\n",
    "    dLdB3 = (dLdP * dPdB3).sum(axis = 0)\n",
    "    \n",
    "    dM3dO2 = W3.T\n",
    "    dO2dN2 = dev_sigmoid(O2)\n",
    "    dN2dM2 = np.ones_like(M2)\n",
    "    dM2dW2 = O1.T\n",
    "    dN2dB2 = np.ones_like(B2)\n",
    "    \n",
    "#          (10, 23)       (23, 1)      (23, 1)     (1, 5)     (23, 5)    (23, 5)\n",
    "#     print(dM2dW2.shape,dLdP.shape,dPdM3.shape,dM3dO2.shape,dO2dN2.shape,dN2dM2.shape)\n",
    "    \n",
    "    dLdW2 = dM2dW2 @ ((((dLdP * dPdM3) @ dM3dO2) * dO2dN2) *dN2dM2 )\n",
    "    dLdB2 = (((dLdP * dPdM3) * dO2dN2) * dN2dB2).sum(axis = 0)\n",
    "    \n",
    "    dM1dO1 = W2.T\n",
    "    dO1dN1 = dev_leaky_relu(O1)\n",
    "    dN1dM1 = np.ones_like(M1)\n",
    "    dM1dW1 = X.T\n",
    "    dN1dB1 = np.ones_like(B1)\n",
    "    \n",
    "#          (13, 506)     (23, 1)    (23, 1)     (1, 5)      (23, 5)      (23, 5)     (5, 10)       (23, 10)       (23, 10)\n",
    "#     print(dM1dW1.shape,dLdP.shape,dPdM3.shape,dM3dO2.shape,dO2dN2.shape,dN2dM2.shape,dM1dO1.shape, dO1dN1.shape,dN1dM1.shape)\n",
    "    dLdW1 = dM1dW1 @ (((((((dLdP * dPdM3) @ dM3dO2) * dO2dN2) *dN2dM2 ) @ dM1dO1) * dO1dN1) * dN1dM1)\n",
    "    dLdB1 = (((((((dLdP * dPdM3) @ dM3dO2) * dO2dN2) *dN2dM2 ) @ dM1dO1) * dO1dN1) * dN1dB1).sum(axis = 0)\n",
    "    \n",
    "    loss_gradients: Dict[str,ndarray] = {}\n",
    "    loss_gradients['W3'] = dLdW3\n",
    "    loss_gradients['B3'] = dLdB3\n",
    "    loss_gradients['W2'] = dLdW2\n",
    "    loss_gradients['B2'] = dLdB2\n",
    "    loss_gradients['W1'] = dLdW1\n",
    "    loss_gradients['B1'] = dLdB1\n",
    "    return loss_gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X:ndarray,\n",
    "         y:ndarray,\n",
    "         n_iter: int = 1000,\n",
    "         learning_rate:float = 0.01,\n",
    "         batch_size: int = 100,\n",
    "         return_losses: bool = False,\n",
    "         return_weights: bool = False,\n",
    "         hidden_size1 = 10,\n",
    "         hidden_size2 = 5) -> None:\n",
    "    np.random.seed(42)\n",
    "    start = 0\n",
    "    \n",
    "    #initialize random weights\n",
    "    weights = init_weights(X_train.shape[1],hidden_size1 = hidden_size1, hidden_size2 = hidden_size2)\n",
    "    \n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    x,y = X[perm],y[perm]\n",
    "    \n",
    "    if return_losses:\n",
    "        losses = []\n",
    "    \n",
    "    # in case all data is used\n",
    "    for i in range(n_iter):\n",
    "        if start >= X.shape[0]:\n",
    "            perm = np.random.permutation(X.shape[0])\n",
    "            x,y = X[perm],y[perm]\n",
    "            start = 0\n",
    "        \n",
    "        #reduce batch size if exceeds:\n",
    "        if start >=  X.shape[0]:\n",
    "            batch_size = X.shape[0] - start\n",
    "        \n",
    "        X_batch,y_batch = X[start:start+batch_size],y[start:start+batch_size]\n",
    "        start += batch_size\n",
    "        \n",
    "        #train net using generated batch\n",
    "        forward_info, loss = forward(X_batch,y_batch,weights)\n",
    "        \n",
    "        if return_losses:\n",
    "            losses.append(loss)\n",
    "        \n",
    "        loss_grads = loss_gradients(forward_info,weights)\n",
    "        \n",
    "        #loss_grads and weights have same keys\n",
    "        for key in weights.keys():\n",
    "            weights[key] -= learning_rate * loss_grads[key]\n",
    "        \n",
    "    if return_weights:\n",
    "        return losses, weights\n",
    "    \n",
    "    return None\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13)\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "X,y = load_boston(return_X_y = True)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train,X_test, y_train,y_test = train_test_split(X,y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape)\n",
    "\n",
    "y_train,y_test = y_train.reshape(-1,1), y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-95-142a3ebd3f07>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-1.0*x))\n"
     ]
    }
   ],
   "source": [
    "train_info = train(X_train,y_train,\n",
    "                   n_iter = 10000,\n",
    "                   learning_rate = 0.001,\n",
    "                   batch_size = 23, \n",
    "                   return_losses = True,\n",
    "                   return_weights = True)\n",
    "losses = train_info[0]\n",
    "weights = train_info[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[-4.82939595e+00, -1.68741039e+00, -2.80818810e+00,\n",
      "         7.81697461e+00,  8.03743193e+00, -8.99364554e-01,\n",
      "        -8.37463349e-01, -5.20299413e+00, -3.78229284e+00,\n",
      "        -1.02481677e+00],\n",
      "       [-7.40740274e+00,  1.07276790e+00, -3.27782504e+00,\n",
      "         2.38930883e+00,  3.41123794e-01,  4.02800446e+00,\n",
      "        -1.68731195e+00,  9.94420943e+00, -1.03527714e+00,\n",
      "         1.25416503e+00],\n",
      "       [-2.64093882e+00,  2.29114055e+00, -1.60393469e+00,\n",
      "        -3.63500790e+00, -6.44118456e+00,  2.34491848e+00,\n",
      "        -1.70672221e+00,  2.26063818e+00, -1.41861528e+00,\n",
      "        -4.39045332e-01],\n",
      "       [-2.89035186e+00,  1.08228243e+00, -1.55015032e+00,\n",
      "        -1.37875437e+00, -2.61121214e+00, -6.13815867e+00,\n",
      "         2.68688884e-01,  4.70061809e+00, -1.12183730e+00,\n",
      "        -3.06825052e+00],\n",
      "       [-8.16036925e-01, -1.92024897e-01, -5.40222650e-01,\n",
      "         1.66081294e+00, -7.40477261e+00,  1.32624214e+00,\n",
      "        -4.43486068e-01,  8.67805681e-01,  2.59185277e-01,\n",
      "         1.41692200e+00],\n",
      "       [ 2.11478173e+00,  2.08074250e-01,  9.98870608e-01,\n",
      "        -3.53209596e+00, -1.21101510e+00, -1.89468839e-01,\n",
      "        -9.57318637e-02, -1.02041168e+00, -4.98444071e-01,\n",
      "        -1.20496394e+00],\n",
      "       [ 3.61389527e+00, -9.81406586e+00,  1.33835743e+00,\n",
      "        -9.72141696e+00, -3.46350571e+00, -9.96171152e+00,\n",
      "         4.12866559e-01, -1.96660985e+01, -8.03207500e-01,\n",
      "        -3.77574203e+00],\n",
      "       [ 2.78934965e-01, -3.45659850e+00, -3.28784395e-01,\n",
      "         1.63186701e+00, -1.24367278e+00, -4.97449377e+00,\n",
      "         7.11908789e-03,  3.76611453e+00,  2.44818111e+00,\n",
      "        -3.10269204e+00],\n",
      "       [-2.35436929e+00, -2.15233620e+00, -8.93808554e-02,\n",
      "        -2.00712027e+00, -2.48675270e-01, -3.74811355e+00,\n",
      "        -1.00736694e-01,  6.38893083e+00, -9.34084082e-01,\n",
      "        -1.54221267e+00],\n",
      "       [-1.81333460e+00, -6.29828976e+00, -1.82300399e+00,\n",
      "        -9.38178395e-01, -4.06620542e+00,  1.16375214e+00,\n",
      "        -4.58274230e-01,  3.48333057e+00,  3.98488603e-01,\n",
      "        -1.22434239e+00],\n",
      "       [ 3.18822083e+00,  3.28274450e+00,  1.84989622e+00,\n",
      "        -2.91084860e+00, -4.86038926e+00,  6.52628885e+00,\n",
      "         9.74305023e-01, -4.00397076e+00,  3.43386846e+00,\n",
      "        -2.39496271e-01],\n",
      "       [-9.24304134e-01, -3.22383684e+00, -2.71406376e-01,\n",
      "        -2.68623913e+00, -8.30069704e+00,  9.18204750e-01,\n",
      "        -1.14665982e-01, -1.10645966e+00,  2.01913743e+00,\n",
      "         1.20613537e-02],\n",
      "       [-1.84925113e+00,  1.23637666e+00, -5.56101892e-01,\n",
      "         2.76848502e+00,  1.51974456e+00, -3.02957177e+00,\n",
      "        -7.45090190e-01,  7.28334770e+00, -6.21899120e-01,\n",
      "         1.98640580e+00]]), 'B1': array([[-24.82250851,   1.45640096, -13.60596131,  -0.24623288,\n",
      "         -0.74359362,   1.82640668,  -5.10502565,   0.84765415,\n",
      "         -2.08718492,  -5.25443841]]), 'W2': array([[-10.41563505,  -1.38166546, -12.18152326,   1.12777598,\n",
      "          1.7372536 ],\n",
      "       [ -0.18657306,  -2.18832702,   0.7365651 ,  -0.82377447,\n",
      "          0.5357349 ],\n",
      "       [ -5.49446661,  -1.0124663 ,  -6.34071579,   0.80248638,\n",
      "          1.2521593 ],\n",
      "       [  1.53597715,   0.10932394,   1.6738843 ,  -1.58736336,\n",
      "          0.61334223],\n",
      "       [ -1.85953074,  -0.76983604,  -0.97430283,  -1.48989411,\n",
      "          0.33340185],\n",
      "       [ -0.64715204,  -1.68383028,   0.35251956,  -1.92274022,\n",
      "         -1.76814847],\n",
      "       [ -2.45898434,  -1.06028383,  -1.8700139 ,   1.06522739,\n",
      "          0.65025127],\n",
      "       [  6.4946934 ,  -1.97334476,   5.01312438,  -1.94171918,\n",
      "         -0.88294515],\n",
      "       [  0.35519815,  -1.09599214,  -0.38065588,   0.89215402,\n",
      "         -1.0772169 ],\n",
      "       [  0.04322366,   0.83036526,   0.37073838,   0.32021775,\n",
      "         -0.07570259]]), 'B2': array([[5.21368615, 6.46010513, 5.88336545, 4.43961865, 5.74724605]]), 'W3': array([[ 0.13416751],\n",
      "       [ 0.21185282],\n",
      "       [-0.12603172],\n",
      "       [-0.1261746 ],\n",
      "       [-0.05615281]]), 'B3': array([[22.82484885]])}\n"
     ]
    }
   ],
   "source": [
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X:ndarray,\n",
    "           weights: Dict[str,ndarray]) -> ndarray:\n",
    "    M1 = X @ weights['W1']\n",
    "    N1 = M1 + weights['B1']\n",
    "    O1 = leaky_relu(N1)\n",
    "    M2 = O1 @ weights['W2']\n",
    "    N2 = M2 + weights['B2']\n",
    "    O2 = sigmoid(N2)\n",
    "    M3 = O2 @ weights['W3']\n",
    "    P = M3 + weights['B3']\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(X_test,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.8207267163059\n",
      "-0.028306825674809133\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(y_test,y_pred))\n",
    "print(r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
