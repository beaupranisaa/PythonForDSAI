{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Another popular one is Gradient Boosting.  Similar to AdaBoost, Gradient Boosting works by adding sequential predictors.  However, instead of adding **weights**, this method tries to fit the new predictor to the **residual errors** made by the previous predictor.    The hypothesis function of gradient boosting is as follows:\n",
    "\n",
    "$$\n",
    "H(x) = h_0(x) + \\alpha_1h_1(x) + \\cdots + \\alpha_sh_s(x)\n",
    "$$\n",
    "\n",
    "Although they look similar, notice that no alpha is applied to the first predictor.  In addition, each alpha is the same, as opposed to voting power in AdaBoost.  Typically, similar to AdaBoost, decision trees are used for each $h_i(x)$ but are not limited to stump.  In practice, min_leaves are set to around 8 to 32.\n",
    "\n",
    "Since gradient boosting actually originate from additive linear regression, we shall first talk about **gradient boosting for regression**.  Also assume that we are using **regression trees** for our regressors.\n",
    "\n",
    "### Gradient Boosting for Regression\n",
    "\n",
    "Firstly, let's look at the following equation where $h_0(x)$ is our first predictor and we would like to minimize the residual as follows:\n",
    "\n",
    "$$h_0(x) + residual_0 = y $$\n",
    "$$ residual_0 =  y - h_0(x) $$\n",
    "\n",
    "That is, we would $y$ to be as close as $h_0(x)$ such that residual is 0\n",
    "\n",
    "$$ y = h_0(x) $$\n",
    "\n",
    "The question is that is it possible to add the second predictor $h_1(x)$ such that the residual is further reduced\n",
    "\n",
    "$$ y = h_0(x) + h_1(x) $$\n",
    "\n",
    "This equation can be written as:\n",
    "\n",
    "$$h_1(x) = y - h_0(x) $$\n",
    "\n",
    "This equation informs us that if we can find a subsequent predictor that can best fit the \"residual\" (i.e. $y - h_0(x)$), then we can improve the accuracy.\n",
    "\n",
    "**How is this related to gradient descent?**\n",
    "\n",
    "Well, firstly, here is our loss function for regression:\n",
    "\n",
    "$$J = \\frac{1}{2}(y - h(x))^2$$\n",
    "\n",
    "And here, we want to minimize $J$ by gradient of the loss function in respect to by adjusting $h_x$.  We can thus treat $h_x$ as parameters and take derivatives:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial h_(x)} = y - h(x)$$\n",
    "\n",
    "Thus, we can interpret residuals as negative gradients:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "y & = h_0(x) + h_1(x)\\\\\n",
    "& = h_0(x) + (y - h_0(x)) \\\\\n",
    "& = h_0(x) - (h_0(x) - y) \\\\\n",
    "& = h_0(x) - \\frac{\\partial J}{\\partial h_0(x)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So in fact, we are using \"gradient\" descent in \"gradient\" boosting to find the new model, written as:\n",
    "\n",
    "$$h_1(x) = - \\frac{\\partial J}{\\partial h_0(x)} = y - h_0(x)$$\n",
    "\n",
    "or more generally\n",
    "\n",
    "$$h_s(x) = - \\frac{\\partial J}{\\partial h_{s-1}(x)} = y - h_{s-1}(x)$$\n",
    "\n",
    "where $s$ is the index of predictor\n",
    "\n",
    "**So residuals or gradients?**\n",
    "\n",
    "Although they are equivalent in the mse loss function, it is more useful to use negative gradients as it is more general, and can apply to other loss functions as well, e.g., cross-entropy in the case of classification.\n",
    "\n",
    "In cross entropy, the loss function is\n",
    "\n",
    " $$J= y log h(x) + (1 - y) lg (1-h(x))$$\n",
    " \n",
    "If you look at our previous lecture on logistic regression, the derivative of this **in respect to h(x)** will be:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial h_(x)} = y - h(x)$$\n",
    "\n",
    "This may look the same as mse, but here, h(x) is\n",
    "\n",
    "$$h(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "\n",
    "**Adding learning rate**\n",
    "\n",
    "To make sure adding the subsequent predictor would not overfit our model, we shall add an learning rate $\\alpha$ in front of this, which shall be the same across all predictors (different from AdaBoost where alpha is different across all predictors)\n",
    "\n",
    "$$h_s(x) = - \\alpha \\frac{\\partial J}{\\partial h_{s-1}(x)}$$\n",
    "\n",
    "**What about next predictor**\n",
    "\n",
    "We can stop if we are happy, either using some predefined iterations, or whether the residual does not decrease further using some validation set.  \n",
    "\n",
    "In this case, it is obvious that 2 predictors are simply not enough.  Thus, we first need to calculate the residuals which are\n",
    "\n",
    "$$ residual_1 =  y - (h_0(x) + \\alpha h_1(x))$$\n",
    "\n",
    "then we define $h_2(x)$ as \n",
    "\n",
    "$$h_2(x) = \\alpha(y - (h_0(x) + \\alpha h_1(x)))$$\n",
    "\n",
    "And then repeat\n",
    "\n",
    "The final prediction shall use the following hypothesis function $H(x)$:\n",
    "\n",
    "$$\n",
    "H(x) = h_0(x) + \\alpha_1h_1(x) + \\cdots + \\alpha_sh_s(x)\n",
    "$$\n",
    "\n",
    "**Summary of steps**\n",
    "\n",
    "1. Initialize the model as simply mean or some constant\n",
    "2. Predict and calculate the residual\n",
    "3. Let the next model fit the residual\n",
    "4. Predict using the combined models and calculate the residual\n",
    "5. Let the next model fit this residual\n",
    "6. Simply repeat 4-5 until stopping criteria is reached\n",
    "\n",
    "### Gradient Boosting for Classification\n",
    "\n",
    "What we have discussed is gradient boosting for regression.  However, there are not much difference for classification.  \n",
    "\n",
    "Recall the cost function of classification is **cross entropy** where its derivative is simply:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial h_(x)} = y - h(x)$$\n",
    "\n",
    "Although this may look similar, $h(x)$ carries a function that convert real value to probabilities.\n",
    "\n",
    "For binary classification, $h(x)$ is defined as the sigmoid function:\n",
    "\n",
    "$$h(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "For multiclass/binary classification, $h(x)$ is defined as the softmax function:\n",
    "\n",
    "$$h(x) = \\frac{e^x_c}{\\Sigma_{i=1}^{k} e^x_k}$$\n",
    "\n",
    "Also remind that to use softmax function, we need to first one-hot encode our y.  And during prediction, we need to perform <code>np.argmax</code> along the axis=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "def grad(y, f):\n",
    "    return y - f #positive later on we will minus., if we use f-y then we dont need to minus later on\n",
    "\n",
    "def fit(X, y, models):\n",
    "    \n",
    "    models_trained = [] # the one i already fitted, dont know the size so use []\n",
    "    \n",
    "    #using DummyRegressor is a good technique for starting model\n",
    "    first_model = DummyRegressor(strategy='mean') # or (\"constant\" set to 0)\n",
    "    first_model.fit(X, y)\n",
    "    models_trained.append(first_model)\n",
    "    \n",
    "    #fit the estimators\n",
    "    for i, model in enumerate(models):\n",
    "        #predict using all the weak learners we trained up to\n",
    "        #this point\n",
    "        y_pred = predict(X, models_trained) #not sklearn predict \n",
    "        \n",
    "        #errors will be the total errors maded by models_trained\n",
    "        residual = grad(y, y_pred) #negative gradient # combined all the mdodel????????\n",
    "        \n",
    "        #fit the next model with residual\n",
    "        model.fit(X, residual) #next model tries to fit the residual\n",
    "        \n",
    "        models_trained.append(model)\n",
    "        \n",
    "    return models_trained\n",
    "        \n",
    "def predict(X, models):\n",
    "    learning_rate = 0.1  ##hard code for now\n",
    "    f0 = models[0].predict(X)  #first use the dummy model\n",
    "    boosting = sum(learning_rate * model.predict(X) for model in models[1:]) # if use f-y then use neg sum()\n",
    "    '''\n",
    "    H(X) = h0(X) + alpha*h1(X) + alpha*h2(X)......\n",
    "    '''\n",
    "    return f0 + boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our MSE:  12.945557601580582\n"
     ]
    }
   ],
   "source": [
    "# Regression\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, random_state=42)\n",
    "\n",
    "n_estimators = 200\n",
    "tree_params = {'max_depth': 1} #stump\n",
    "models = [DecisionTreeRegressor(**tree_params) for _ in range(n_estimators)]\n",
    "\n",
    "#fit the models\n",
    "models = fit(X_train, y_train, models)\n",
    "\n",
    "#predict\n",
    "y_pred = predict(X_test, models)\n",
    "\n",
    "#print metrics\n",
    "print(\"Our MSE: \", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn \n",
    "\n",
    "sklearn has implemented GradientBoosting under the API of <code>GradientBoostingClassifier</code> for classification and <code>GradientBoostingRegressor</code> for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn MSE:  12.945557601580584\n"
     ]
    }
   ],
   "source": [
    "#Compare to sklearn: ls is the same as our mse\n",
    "sklearn_model = GradientBoostingRegressor(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate = 0.1,\n",
    "    max_depth=1,\n",
    "    loss='ls'\n",
    ")\n",
    "\n",
    "y_pred_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#print metrics\n",
    "print(\"Sklearn MSE: \", mean_squared_error(y_test, y_pred_sk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost (BEST, better than gradient boosting (X = extreme gradient boosting)\n",
    "\n",
    "Check out LightGMD!!!!!\n",
    "\n",
    "XGBoost is an optimized distributed gradient boosting, designed to be more efficient, flexible, and portable (Chen and Guestrin 2016).  In fact, XGBoost is often an important component of the winning entries in ML competitions (e.g., Kaggle).  XGBoost also offers several nice features, such as automatically taking care of early stopping: XGBoost’s API is quite similar to Scikit-Learn’s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:16.15458\n",
      "Will train until validation_0-rmse hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-rmse:11.84377\n",
      "[2]\tvalidation_0-rmse:8.79602\n",
      "[3]\tvalidation_0-rmse:6.72584\n",
      "[4]\tvalidation_0-rmse:5.46526\n",
      "[5]\tvalidation_0-rmse:4.65454\n",
      "[6]\tvalidation_0-rmse:4.08462\n",
      "[7]\tvalidation_0-rmse:3.76129\n",
      "[8]\tvalidation_0-rmse:3.54313\n",
      "[9]\tvalidation_0-rmse:3.37742\n",
      "[10]\tvalidation_0-rmse:3.24836\n",
      "[11]\tvalidation_0-rmse:3.18872\n",
      "[12]\tvalidation_0-rmse:3.10860\n",
      "[13]\tvalidation_0-rmse:3.09993\n",
      "[14]\tvalidation_0-rmse:3.08393\n",
      "[15]\tvalidation_0-rmse:3.08760\n",
      "[16]\tvalidation_0-rmse:3.06310\n",
      "[17]\tvalidation_0-rmse:3.05292\n",
      "[18]\tvalidation_0-rmse:3.05715\n",
      "[19]\tvalidation_0-rmse:3.05827\n",
      "Stopping. Best iteration:\n",
      "[17]\tvalidation_0-rmse:3.05292\n",
      "\n",
      "MSE: 9.320308418219375\n"
     ]
    }
   ],
   "source": [
    "#make sure to pip install xgboost\n",
    "#for mac guys, do \"brew install libomp\" which installs openMP library\n",
    "#required for XGBoost\n",
    "\n",
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor() \n",
    "\n",
    "#not improved after 2 iterations\n",
    "xgb_reg.fit(X_train, y_train,\n",
    "                eval_set=[(X_test, y_test)], early_stopping_rounds=2)\n",
    "y_pred = xgb_reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE:\", mse)  #notice we are using mse while xgb uses root mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 5.29 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "159 ms ± 84.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "102 ms ± 12 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit xgboost.XGBRegressor().fit(X_train, y_train)\n",
    "%timeit GradientBoostingRegressor().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===Task===\n",
    "\n",
    "Modify the above scratch code such that:\n",
    "- Notice that we are still using max_depth = 1.  Attempt to tweak min_samples_split, max_depth for the regression and see whether we can achieve better mse on our boston data\n",
    "- Notice that we only write scratch code for gradient boosting for regression, add some code so that it also works for binary classification.  Load the breast cancer data from sklearn and see that it works.\n",
    "- Further change the code so that it works for multiclass classification.  Load the digits data from sklearn and see that it works\n",
    "- Put everything into class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "class GradientBoosting:\n",
    "    def __init__(self, S=5, learning_rate=1, max_depth = 1, \n",
    "                 min_samples_split = 2,\n",
    "                 regression=True, tol=1e-4):\n",
    "        self.S = S\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.regression=regression\n",
    "            \n",
    "        #initialize regression trees\n",
    "        tree_params = {'max_depth': self.max_depth,\n",
    "                      'min_samples_split': self.min_samples_split}\n",
    "        self.models = [DecisionTreeRegressor(**tree_params) for _ in range(S)]        \n",
    "        first_model = DummyRegressor(strategy='mean')\n",
    "        self.models.insert(0, first_model)\n",
    "        \n",
    "    def grad(self, y, h):\n",
    "        return y - h\n",
    "    \n",
    "    def fit(self, X, y):  #<----X_train\n",
    "        \n",
    "        #fit the first model\n",
    "        self.models[0].fit(X, y)\n",
    "        \n",
    "        for i in range(self.S):\n",
    "            #predict\n",
    "            yhat = self.predict(X, self.models[:i+1], with_argmax=False)\n",
    "            \n",
    "            #get the gradient\n",
    "            gradient = self.grad(y, yhat)\n",
    "            \n",
    "            #fit the next model with gradient\n",
    "            self.models[i+1].fit(X, gradient)\n",
    "    \n",
    "    def predict(self, X, models=None, with_argmax=True):\n",
    "        if models is None:\n",
    "            models = self.models\n",
    "        h0 = models[0].predict(X)  #first use the dummy model\n",
    "        boosting = sum(self.learning_rate * model.predict(X) for model in models[1:])\n",
    "        yhat = h0 + boosting\n",
    "        if not self.regression:\n",
    "            #turn into probability using softmax\n",
    "            yhat = np.exp(yhat) / np.sum(np.exp(yhat), axis=1, keepdims=True)\n",
    "            if with_argmax:\n",
    "                yhat = np.argmax(yhat, axis=1)\n",
    "        return yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  7.835738712397973\n",
      "Sklearn MSE:  7.922065937623081\n"
     ]
    }
   ],
   "source": [
    "# Regression\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                        test_size=0.3, random_state=42)\n",
    "\n",
    "model = GradientBoosting(S=200, learning_rate=0.1, max_depth = 3, \n",
    "                 min_samples_split = 2,\n",
    "                 regression=True, tol=1e-4)\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "#print metrics\n",
    "print(\"MSE: \", mean_squared_error(y_test, yhat))\n",
    "\n",
    "#=====SKlearn========\n",
    "#Compare to sklearn: ls is the same as our mse\n",
    "sklearn_model = GradientBoostingRegressor(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate = 0.1,\n",
    "    max_depth=3,\n",
    "    loss='ls'\n",
    ")\n",
    "\n",
    "yhat_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Sklearn MSE: \", mean_squared_error(y_test, yhat_sk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our accuracy:  0.9649122807017544\n",
      "Sklearn accuracy:  0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "# Binary classification\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "y_train_encoded = np.zeros((y_train.shape[0], len(set(y))))\n",
    "for each_class in range(len(set(y))):\n",
    "    cond = y_train==each_class\n",
    "    y_train_encoded[np.where(cond), each_class] = 1\n",
    "\n",
    "model = GradientBoosting(S=200, learning_rate=0.1, max_depth = 3, \n",
    "                 min_samples_split = 2,\n",
    "                 regression=False)\n",
    "model.fit(X_train, y_train_encoded)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# #print metrics\n",
    "print(\"Our accuracy: \", accuracy_score(y_test, yhat))\n",
    "\n",
    "#=====SKlearn========\n",
    "#Compare to sklearn: ls is the same as our accuracy\n",
    "sklearn_model = GradientBoostingClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate = 0.1,\n",
    "    max_depth=1\n",
    ")\n",
    "\n",
    "yhat_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Sklearn accuracy: \", accuracy_score(y_test, yhat_sk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our accuracy:  0.9314814814814815\n",
      "Sklearn accuracy:  0.9481481481481482\n"
     ]
    }
   ],
   "source": [
    "# Multiclass classification\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "y_train_encoded = np.zeros((y_train.shape[0], len(set(y))))\n",
    "for each_class in range(len(set(y))):\n",
    "    cond = y_train==each_class\n",
    "    y_train_encoded[np.where(cond), each_class] = 1\n",
    "\n",
    "model = GradientBoosting(S=200, learning_rate=0.1, max_depth = 3, \n",
    "                 min_samples_split = 2,\n",
    "                 regression=False)\n",
    "model.fit(X_train, y_train_encoded)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# #print metrics\n",
    "print(\"Our accuracy: \", accuracy_score(y_test, yhat))\n",
    "\n",
    "#=====SKlearn========\n",
    "#Compare to sklearn: ls is the same as our accuracy\n",
    "sklearn_model = GradientBoostingClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate = 0.1,\n",
    "    max_depth=1\n",
    ")\n",
    "\n",
    "yhat_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Sklearn accuracy: \", accuracy_score(y_test, yhat_sk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST data, and split it into a training set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for validation, and 10,000 for testing). Then train various classifiers, such as a Random Forest classifier, a Logistic Regression classifier, an SVM, and a MLPClassifier (I haven't taught you yet, but its a simple neural network with multi-layers). \n",
    " \n",
    "Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier. Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?\n",
    " \n",
    "Last, attemp to use XGBoost.  Does it improve the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import fetch_openml\n",
    "# import numpy as np\n",
    "\n",
    "# #fetching\n",
    "# mnist = fetch_openml('mnist_784', version=1)\n",
    "\n",
    "# #make sure is int\n",
    "# mnist.target = mnist.target.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Load the MNIST data and split it into a training set, a validation set, and \n",
    "#a test set (e.g., use 50,000 instances for training, 10,000 for \n",
    "#validation, and 10,000 for testing)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    mnist.data, mnist.target, test_size=10000, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", probability=True, random_state=42)\n",
    "mlp_clf = MLPClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [rnd_clf, log_clf, svm_clf, mlp_clf]\n",
    "for model in models:\n",
    "    print(\"Training the\", model.__class__.__name__)\n",
    "    model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[model.score(X_val, y_val) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "named_estimators = [\n",
    "    (\"rnd_clf\", rnd_clf),\n",
    "    (\"log_clf\", log_clf),\n",
    "    (\"svm_clf\", svm_clf),\n",
    "    (\"mlp_clf\", mlp_clf),\n",
    "]\n",
    "\n",
    "voting_clf = VotingClassifier(named_estimators)\n",
    "voting_clf.fit(X_train, y_train)\n",
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's print out the individual estimator\n",
    "[estimator.score(X_val, y_val) for estimator in voting_clf.estimators_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del voting_clf.estimators_[1]  #second estimator\n",
    "\n",
    "#print estimators to make sure its deleted\n",
    "voting_clf.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the score again\n",
    "voting_clf.score(X_val, y_val)  #yay increase a little!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf.voting = \"hard\"\n",
    "print(\"Voting classifier score:\", voting_clf.score(X_test, y_test))\n",
    "print(\"Each classifier scor: \")\n",
    "[estimator.score(X_test, y_test) for estimator in voting_clf.estimators_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_clf = xgboost.XGBClassifier() \n",
    "\n",
    "#not improved after 2 iterations\n",
    "xgb_clf.fit(X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
    "print(\"Score: \", xgb_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use Boosting\n",
    "\n",
    "Let's summarize some useful info about Gradient Boosting:\n",
    "\n",
    "Advantages:\n",
    "1. Extremely powerful - especially useful for heterogeneous data (e.g., house price, number of bedrooms). \n",
    "\n",
    "Disadvantages:\n",
    "1. They cannot be parallelized.  Obvious since they are sequential predictors.\n",
    "2. They can easily overfit, thus require careful choice of estimators or the use of regularization such as max_depth.\n",
    "3. When we talk about homogeneous data such as images, videos, audio, text, or huge amount of data, deep learning works better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
