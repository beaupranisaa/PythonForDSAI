{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, Tuple, List\n",
    "from numpy import ndarray\n",
    "import numpy as np\n",
    "#weights are dict with str as key and value as ndarray\n",
    "#forward will return two values, first value is a float\n",
    "#and second value is the weight\n",
    "\n",
    "def init_weights(inputsize:int,hiddensize:int, outputsize:int) -> Dict[str,ndarray]:\n",
    "    weights: Dict[str,ndarray] = {}\n",
    "#     weights['W'] = np.full((n_in,1),1/n_in)\n",
    "    weights['W1'] = np.random.randn(inputsize,hiddensize)\n",
    "    weights['W2'] = np.random.randn(hiddensize,outputsize)\n",
    "    weights['B1'] = np.random.randn(1,hiddensize)\n",
    "    weights['B2'] = np.random.randn(1,1)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def forward(X: ndarray,y:ndarray,weights:Dict[str,ndarray])  -> Tuple[float,Dict[str,ndarray]]: # Tuple(forward info, loss)\n",
    "    \n",
    "    # weights['B']\n",
    "    # weights ['W']\n",
    "    \n",
    "#     print(weights['W'].shape)\n",
    "    \n",
    "    ### Assert batch sizes of X and y are equal\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    ### Assert that X and w can be dotted\n",
    "    assert X.shape[1] == weights['W1'].shape[0]\n",
    "    \n",
    "    ### Assert that B is just a value (is shape (1,1))\n",
    "    assert weights['B1'].shape[0] == 1\n",
    "    assert weights['B1'].shape[1] == weights['W1'].shape[1]\n",
    "    assert weights['B2'].shape[0] == 1\n",
    "    assert weights['B2'].shape[1] == 1\n",
    "    \n",
    "    ### compute M1\n",
    "    M1 = X @ weights['W1']\n",
    "    \n",
    "    ### compute N1\n",
    "    N1 = M1 + weights['B1']\n",
    "    \n",
    "    ### compute sigmoid\n",
    "    O1 = 1/(1+np.exp(-N1))\n",
    "    \n",
    "    ### compute V\n",
    "    M2 = O1@ weights['W2']\n",
    "    \n",
    "    ### compute alpha\n",
    "    P = M2 + weights['B2']\n",
    "    \n",
    "    ### compute L\n",
    "    L = np.mean(np.power(P-y,2))\n",
    "#     print('Loss', L)\n",
    "    \n",
    "    # save the information of N,P,L in a dictionary called forward_inf\n",
    "    forward_info: Dict[str,ndarray] = {} # initializing dictionary with data type specified\n",
    "        \n",
    "    ### set the forward_info to remember X,N,P,y\n",
    "    # for example\n",
    "    #maybe use this for calculating gradients\n",
    "    forward_info['X'] = X\n",
    "    forward_info['M1'] = M1\n",
    "    forward_info['N1'] = N1\n",
    "    forward_info['B1'] = weights['B1']\n",
    "    forward_info['B2'] = weights['B2']\n",
    "    forward_info['O1'] = O1\n",
    "    forward_info['W1'] = weights['W1']\n",
    "    forward_info['W2'] = weights['W2']\n",
    "    forward_info['M2'] = M2\n",
    "    forward_info['P'] = P\n",
    "    forward_info['y'] = y\n",
    "    \n",
    "    return forward_info, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(forward_info: Dict[str,ndarray],weights: Dict[str,ndarray]) -> Dict[str,ndarray]:\n",
    "    \n",
    "#     y = forward_info['y']\n",
    "#     P = forward_info['P']\n",
    "#     N = forward_info['N']\n",
    "#     X = forward_info['X']\n",
    "\n",
    "#     alpha = forward_info['alpha']\n",
    "    y = forward_info['y']\n",
    "    P = forward_info['P']\n",
    "    M2 = forward_info['M2']\n",
    "    W2 = forward_info['W2']\n",
    "    W1 = forward_info['W1']\n",
    "    O1 = forward_info['O1']\n",
    "    B1 = forward_info['B1']\n",
    "    B2 = forward_info['B2']\n",
    "    N1 = forward_info['N1']\n",
    "    M1 = forward_info['M1']\n",
    "    X = forward_info['X']\n",
    "    \n",
    "\n",
    "    dLdP = 2* (P - y) #m,1\n",
    "    \n",
    "    dPdM2 = np.ones_like(M2) #m,1 #shape of M2\n",
    "    \n",
    "    dLdM2 = dLdP * dPdM2 #m,1\n",
    "    \n",
    "    dM2dO1 = W2.T #1,h\n",
    "    \n",
    "    dLdO1 = dLdM2 @ dM2dO1 #m,h\n",
    "    \n",
    "    dO1dN1 = O1*(1-O1) #m,h\n",
    "    \n",
    "    dLdN1 = dLdO1 * dO1dN1 #m,h\n",
    "    \n",
    "    dN1dM1 = np.ones_like(M1) #m,h #shape of M1\n",
    "    \n",
    "    dLdM1 = dLdN1 * dN1dM1 #m,h  \n",
    "    \n",
    "    dM1dW1 = X.T #n,mq  \n",
    "    \n",
    "    dLdW1 = dM1dW1 @ dLdM1 #\n",
    "    \n",
    "    dN1dB1 = np.ones_like(B1) #shape of B1\n",
    "    \n",
    "    dLdB1 = (dLdN1 * dN1dB1).sum(axis = 0)\n",
    "    \n",
    "    dM2dW2 = O1.T #h,m \n",
    "    \n",
    "    dLdW2 = dM2dW2  @ dLdM2 \n",
    "    \n",
    "    dPdB2 = np.ones_like(B2) #shape of B2\n",
    "    \n",
    "    dLdB2 = (dLdP * dPdB2).sum(axis = 0)\n",
    "\n",
    "    \n",
    "    grads: Dict[str,ndarray] = {}\n",
    "\n",
    "    grads['W2'] = dLdW2  \n",
    "    grads['B2'] = dLdB2\n",
    "    grads['W1'] = dLdW1\n",
    "    grads['B1'] = dLdB1\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permuteXY(X, y):\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    return X[perm],y[perm]\n",
    "\n",
    "def train(X: ndarray, y:ndarray, max_iter: int = 1000, learning_rate: float = 0.01, \n",
    "          batch_size: int= 100) -> None: # the weights change #< -- mini-batch gradient descent\n",
    "    np.random.seed(42)\n",
    "    start = 0 #<--  initialize start index for mini-batch (we are gonna do without replacement) # no data will be used more than once -> without replacement\n",
    "    \n",
    "    #get my weights dict\n",
    "    inputsize = X.shape[1]\n",
    "    hiddensize = 13\n",
    "    outputsize = 1\n",
    "    weights = init_weights(inputsize,hiddensize,outputsize) #<-- init_weights look up there^\n",
    "    \n",
    "    #shuffle my X a little bit to increase generalizing power\n",
    "    X,y = permuteXY(X,y)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # in case all data used\n",
    "        # index is exceeded\n",
    "        if start >=  X.shape[0]: \n",
    "            # shuffle X again\n",
    "            X,y = permuteXY(X,y)\n",
    "            # restart the start index\n",
    "            start = 0\n",
    "            \n",
    "        # if batch_size exceeds the last guy, reduce the batch size\n",
    "        if start + batch_size > X.shape[0]:\n",
    "            batch_size = X.shape[0] - start\n",
    "            \n",
    "        X_batch,y_batch = X[start:start+batch_size] , y[start:start+batch_size]\n",
    "        start += batch_size\n",
    "        \n",
    "        # perform first prediction\n",
    "        forward_info, loss = forward(X_batch,y_batch,weights)\n",
    "        \n",
    "        # calculate gradients\n",
    "        loss_grad = backward(forward_info,weights)\n",
    "        \n",
    "        # update W and B\n",
    "        weights['B1'] -= learning_rate * loss_grad['B1']\n",
    "        weights['W1'] -= learning_rate * loss_grad['W1']\n",
    "        weights['B2'] -= learning_rate * loss_grad['B2']\n",
    "        weights['W2'] -= learning_rate * loss_grad['W2']\n",
    "        \n",
    "    return weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### lets load some data\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#### so please load boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "#boston = load_boston()\n",
    "#X = boston.data\n",
    "#y = boston.target\n",
    "\n",
    "#### please standardize them\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "#### train test split them\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size =0.3,random_state = 42)\n",
    "\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "##### reshape y to (m,1) < --- because our code want 1 there\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.1542346 , -0.78919109, -0.20190233,  1.61576053, -0.42313543,\n",
       "         -0.63328183,  0.47762574, -0.18569829, -0.93419023,  0.50038345,\n",
       "         -0.66240852, -0.52412739, -0.24362405],\n",
       "        [-1.49087208, -1.29912297, -0.05100142, -0.79220485,  0.81992276,\n",
       "         -0.81581593, -0.46925406,  0.97960232,  0.27745478,  0.13095733,\n",
       "         -0.19587148,  0.03859562,  0.19217519],\n",
       "        [-0.75816157,  0.42384316, -0.11492181,  0.0052847 , -0.44344517,\n",
       "          2.12819708,  0.97374521, -0.11525674,  0.84299193, -0.96327729,\n",
       "          0.57519418, -2.03176496, -0.83700061],\n",
       "        [-0.11318904,  1.08292305, -0.18994501, -0.26914934,  0.11230511,\n",
       "         -0.3851478 , -0.30017092, -0.0829786 ,  1.05711018,  0.93603477,\n",
       "         -1.03382181,  1.82868086,  0.22395735],\n",
       "        [-0.77664981,  0.64090359,  0.29104044,  1.16934098, -1.42585901,\n",
       "         -0.32804685,  0.707869  ,  0.94368154, -0.63809569,  0.10827806,\n",
       "         -1.45785133, -1.17274395,  0.61427461],\n",
       "        [ 1.86864202,  0.53269335,  0.76536359,  1.3671113 , -0.92075827,\n",
       "          1.14169938,  1.99216573,  0.46860479,  1.6245701 , -1.86131734,\n",
       "          1.09029821,  1.42893742, -0.89243555],\n",
       "        [-0.05280215, -2.01641417, -0.45470892, -0.3210981 ,  1.61568081,\n",
       "         -0.80391255, -1.34807368, -0.22086433,  0.08396169,  0.36073134,\n",
       "         -1.37304181,  0.2108186 ,  0.12492969],\n",
       "        [ 1.1471139 , -0.83580297, -0.05144511, -0.63029595, -1.40871113,\n",
       "          0.20366138,  0.65019923, -0.83408347, -0.39690514, -1.96266929,\n",
       "          0.12597205, -0.59588866, -1.61817668],\n",
       "        [-0.0867544 ,  0.56312966,  1.90095344,  0.73425417,  0.08489493,\n",
       "          0.33062963, -1.51539405,  0.4912627 ,  0.64033956,  2.31031768,\n",
       "         -0.73558549,  0.35859532,  0.53343011],\n",
       "        [-1.20391432,  1.02037422,  0.07169519,  0.95776259, -1.46029773,\n",
       "          1.64422113, -1.29537297,  0.59976871,  1.51463536, -1.21992798,\n",
       "         -0.41226498, -0.42285404, -0.40771601],\n",
       "        [-1.6877085 , -0.35865304, -0.32828057,  0.36182901,  0.38263461,\n",
       "          1.49833224, -0.74753141, -0.23918601,  0.67707027, -1.80633157,\n",
       "         -0.90600484,  0.95990035, -1.28325397],\n",
       "        [ 0.25945837,  0.45219059,  0.28570297, -1.16310253, -0.12919692,\n",
       "          0.62788054,  0.15100481, -0.17813286,  0.57239991, -0.6336004 ,\n",
       "          0.6840133 ,  0.32852829, -0.34185564],\n",
       "        [ 1.53238516, -0.16518734, -2.64806178,  0.51731152, -2.31272129,\n",
       "          0.10856949,  1.24399714, -2.64918198, -1.70815808,  0.06875044,\n",
       "          0.03304089,  1.13811495, -1.43034721]]),\n",
       " 'W2': array([[2.60790936],\n",
       "        [1.31389368],\n",
       "        [4.78904133],\n",
       "        [2.34347623],\n",
       "        [6.03394005],\n",
       "        [1.64991248],\n",
       "        [5.13631196],\n",
       "        [5.01618749],\n",
       "        [5.71300321],\n",
       "        [0.77165368],\n",
       "        [6.17114585],\n",
       "        [3.25091178],\n",
       "        [3.41154868]]),\n",
       " 'B1': array([[-1.8874545 ,  0.10323846, -2.51830257,  0.86433601,  0.71173076,\n",
       "          0.33853873, -2.78024917, -2.88702506, -2.00063543,  0.67355912,\n",
       "          0.52611217, -1.68426443, -1.28627907]]),\n",
       " 'B2': array([[4.53253255]])}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = train(X_train,y_train,max_iter = 10000, learning_rate = 3e-4, batch_size = 20)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def predict(X: ndarray, weights: Dict[str, ndarray]):\n",
    "    O1 = sigmoid(X @ weights['W1'] + weights['B1'])\n",
    "    return O1 @ weights['W2'] + weights['B2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "ypred = predict(X_test,weights)\n",
    "mean_s_r = mean_squared_error(y_test,ypred)\n",
    "r2 =r2_score(y_test,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.200716061748745\n",
      "0.8362607811584442\n"
     ]
    }
   ],
   "source": [
    "print(mean_s_r)\n",
    "print(r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
