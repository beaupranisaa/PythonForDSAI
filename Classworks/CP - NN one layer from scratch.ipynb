{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, Tuple, List\n",
    "from numpy import ndarray\n",
    "import numpy as np\n",
    "#weights are dict with str as key and value as ndarray\n",
    "#forward will return two values, first value is a float\n",
    "#and second value is the weight\n",
    "\n",
    "def init_weights(n_in:int) -> Dict[str,ndarray]:\n",
    "    weights: Dict[str,ndarray] = {}\n",
    "#     weights['W'] = np.full((n_in,1),1/n_in)\n",
    "    weights['W'] = np.random.randn(n_in,1)\n",
    "    weights['B'] = np.random.randn(1,1)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def forward(X: ndarray,y:ndarray,weights:Dict[str,ndarray])  -> Tuple[float,Dict[str,ndarray]]: # Tuple(forward info, loss)\n",
    "    \n",
    "    # weights['B']\n",
    "    # weights ['W']\n",
    "    \n",
    "#     print(weights['W'].shape)\n",
    "    \n",
    "    ### Assert batch sizes of X and y are equal\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    ### Assert that X and w can be dotted\n",
    "    assert X.shape[1] == weights['W'].shape[0]\n",
    "    \n",
    "    ### Assert that B is just a value (is shape (1,1))\n",
    "    assert weights['B'].shape[0] == 1\n",
    "    assert weights['B'].shape[1] == 1\n",
    "    \n",
    "    ### compute N\n",
    "    N = X @ weights['W']\n",
    "    \n",
    "    ### compute P\n",
    "    P = N + weights['B']\n",
    "    \n",
    "    ### compute L\n",
    "    L = np.mean(np.power(P-y,2))\n",
    "    \n",
    "    # save the information of N,P,L in a dictionary called forward_inf\n",
    "    forward_info: Dict[str,ndarray] = {} # initializing dictionary with data type specified\n",
    "        \n",
    "    ### set the forward_info to remember X,N,P,y\n",
    "    # for example\n",
    "    #maybe use this for calculating gradients\n",
    "    forward_info['X'] = X\n",
    "    forward_info['N'] = N\n",
    "    forward_info['P'] = P\n",
    "    forward_info['y'] = y\n",
    "    \n",
    "    return forward_info, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(forward_info: Dict[str,ndarray],weights: Dict[str,ndarray]) -> Dict[str,ndarray]:\n",
    "    \n",
    "#     y = forward_info['y']\n",
    "#     P = forward_info['P']\n",
    "#     N = forward_info['N']\n",
    "#     X = forward_info['X']\n",
    "    \n",
    "    \n",
    "    dLdP = 2* (forward_info['P']-forward_info['y'])\n",
    "    \n",
    "    dPdN = np.ones_like(forward_info['N'])\n",
    "    \n",
    "    dPdB = np.ones_like(weights['B'])\n",
    "    \n",
    "    dLdN = dLdP * dPdN\n",
    "    \n",
    "    dNdW = forward_info['X'].T\n",
    "    \n",
    "    dLdW = dNdW @ dLdN\n",
    "    \n",
    "    dLdB = (dLdP * dPdB).sum(axis=0)\n",
    "    \n",
    "    grads: Dict[str,ndarray] = {}\n",
    "        \n",
    "    grads['W'] = dLdW\n",
    "    grads['B'] = dLdB\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permuteXY(X, y):\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    return X[perm],y[perm]\n",
    "\n",
    "def train(X: ndarray, y:ndarray, max_iter: int = 1000, learning_rate: float = 0.01, \n",
    "          batch_size: int= 100) -> None: # the weights change #< -- mini-batch gradient descent\n",
    "    np.random.seed(42)\n",
    "    start = 0 #<--  initialize start index for mini-batch (we are gonna do without replacement) # no data will be used more than once -> without replacement\n",
    "    \n",
    "    #get my weights dict\n",
    "    weights = init_weights(X.shape[1]) #<-- init_weights look up there^\n",
    "    \n",
    "    #shuffle my X a little bit to increase generalizing power\n",
    "    X,y = permuteXY(X,y)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # in case all data used\n",
    "        # index is exceeded\n",
    "        if start >=  X.shape[0]: \n",
    "            # shuffle X again\n",
    "            X,y = permuteXY(X,y)\n",
    "            # restart the start index\n",
    "            start = 0\n",
    "            \n",
    "        # if batch_size exceeds the last guy, reduce the batch size\n",
    "        if start + batch_size > X.shape[0]:\n",
    "            batch_size = X.shape[0] - start\n",
    "            \n",
    "        X_batch,y_batch = X[start:start+batch_size] , y[start:start+batch_size]\n",
    "        start += batch_size\n",
    "        \n",
    "        # perform first prediction\n",
    "        forward_info, loss = forward(X_batch,y_batch,weights)\n",
    "        \n",
    "        # calculate gradients\n",
    "        loss_grad = backward(forward_info,weights)\n",
    "        \n",
    "        # update W and B\n",
    "        weights['B'] -= learning_rate * loss_grad['B']\n",
    "        weights['W'] -= learning_rate * loss_grad['W']\n",
    "        \n",
    "    return weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### lets load some data\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#### so please load boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "#boston = load_boston()\n",
    "#X = boston.data\n",
    "#y = boston.target\n",
    "\n",
    "#### please standardize them\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "#### train test split them\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size =0.3,random_state = 42)\n",
    "\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "##### reshape y to (m,1) < --- because our code want 1 there\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = train(X_train,y_train,max_iter = 10000, learning_rate = 3e-4, batch_size = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X:ndarray,weights: Dict[str,ndarray]):\n",
    "    pred = X@weights['W'] + weights['B']\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = predict(X_test,weights)\n",
    "mean_s_r = mean_squared_error(y_test,ypred)\n",
    "r2 =r2_score(y_test,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.508459589737047\n",
      "0.697926124016198\n"
     ]
    }
   ],
   "source": [
    "print(mean_s_r)\n",
    "print(r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
