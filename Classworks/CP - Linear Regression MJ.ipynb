{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type - Bunch\n",
    "#Bunch - dictionary of the data\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "         4.9800e+00],\n",
       "        [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "         9.1400e+00],\n",
       "        [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "         4.0300e+00],\n",
       "        ...,\n",
       "        [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         5.6400e+00],\n",
       "        [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "         6.4800e+00],\n",
       "        [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         7.8800e+00]]),\n",
       " 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "        18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "        15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "        13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "        21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "        35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "        19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "        20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "        23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "        33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "        21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "        20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "        23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "        15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "        17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "        25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "        23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "        32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "        34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "        20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "        26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "        31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "        22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "        42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "        36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "        32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "        20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "        20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "        22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "        21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "        19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "        32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "        18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "        16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "        13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "         7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "        12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "        27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "         8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "         9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "        10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "        15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "        19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "        29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "        20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "        23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]),\n",
       " 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "        'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'),\n",
       " 'DESCR': \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\",\n",
       " 'filename': '/opt/conda/lib/python3.8/site-packages/sklearn/datasets/data/boston_house_prices.csv'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.data\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========STEP 1 : Get X,y in right shape\n",
    "\n",
    "m, n = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num of rows in X as same as num of rows in y\n",
    "assert m == y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize data\n",
    "#mean is 0, varience = 1\n",
    "#why\n",
    "#cause allows us to reach convergence faster\n",
    "#why -> because the values are within smaller range\n",
    "#Thus, the gradients are also within limited range, and NOT go crazy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#======STEP 2 : Almost always, feature scale your data\n",
    "\n",
    "#create object \n",
    "#StandardScaler = class\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# print(X[:,5])\n",
    "X = scaler.fit_transform(X)\n",
    "# print(X[:,5])\n",
    "# print(X.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======STEP 3 : Train test split your data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Closed from\n",
    "\n",
    "$(X^TX)^{-1} X^TY$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to get closed form\n",
    "#(XTX)^-1 X^TY\n",
    "#Simple; Set the d(cost function) = 0\n",
    "#And find the \\theta that satisfy the equation\n",
    "#When we can do such a thing in which we set the d(cost function) = 0\n",
    "#--->When its strictly convave, or strictly convex\n",
    "#---->They have only one local maximum (concave), minimum (convex)\n",
    "#=====STEP 4: Do your computations\n",
    "\n",
    "from numpy.linalg import inv\n",
    "\n",
    "def closed_form(X, y):\n",
    "    return inv(X.T @ X) @ X.T @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#What is the shape of X they want\n",
    "#(num of sample, num of features) -----> correct shape\n",
    "#(num of sample, num of sample)\n",
    "#for the closed form formula\n",
    "#How about the intercept\n",
    "\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "print(type(X_train))\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====STEP 5: Find the theta/weights/beta\n",
    "\n",
    "w = closed_form(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==== STEP 6 : compute the accuracy/loss\n",
    "\n",
    "#6.1 predict ---> \\theta^T X\n",
    "yhat = X_test @ w\n",
    "\n",
    "#if i want to compare yhat and y,i need to make sure thay are the same shape\n",
    "assert y_test.shape == yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.2 get the errors\n",
    "errors = ((y_test - yhat)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2883.3450334241084\n"
     ]
    }
   ],
   "source": [
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(354, 14)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(354,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  20.63701197734828\n",
      "Stop at iteration:  866\n"
     ]
    }
   ],
   "source": [
    "#prepare the data\n",
    "\n",
    "#X_train,X_test have intercepts\n",
    "\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "w = np.zeros(X_train.shape[1])\n",
    "# print(w.shape)\n",
    "\n",
    "alpha = 0.00001\n",
    "max_iter = 2000  #==> typical to call its epochs\n",
    "loss_old = 10000\n",
    "tol = 0.01\n",
    "iter_stop = 0\n",
    "\n",
    "#define your for loop\n",
    "for i in range(max_iter):\n",
    "#     1.yhat  = X@w\n",
    "#     prediction\n",
    "#     yhat () = (m,n)@(n, )\n",
    "    yhat = X_train@w\n",
    "    \n",
    "#     2.error = yhat-y_train\n",
    "#     error for use to calculate gradients\n",
    "#     error () = (m, ) - (m, )\n",
    "    error = yhat - y_train\n",
    "\n",
    "#     2.1.early stopping\n",
    "#     so we dont go through all max_iter iterations\n",
    "#     (yi_hat - yi) ^2\n",
    "#     loss_new(scalar) = (((m, ) - (m, ))**2/m).sum()\n",
    "    loss_new = ((yhat - y_train)**2/yhat.shape[0]).sum() #<<<<mean square error\n",
    "    if np.abs (loss_new-loss_old)<tol: #np.allclose\n",
    "        iter_stop = i\n",
    "        break\n",
    "    loss_old = loss_new\n",
    "    \n",
    "#     3. grad = X.T@error\n",
    "#     grad(n, ) = (n,m) @ (m,)\n",
    "#     grad for each feature \n",
    "    grad = X_train.T@error\n",
    "    \n",
    "#     4. w = w-alpha*grad   \n",
    "#     update w\n",
    "#     #w (n, ) = (n, ) -scalar*(n, )\n",
    "    w = w-alpha*grad\n",
    "\n",
    "# we got our lovely w\n",
    "# now its time to check our accuracy\n",
    "#1. make prediction\n",
    "yhat=X_test@w \n",
    "#2. calculate our mean square error\n",
    "mse = ((yhat - y_test)**2/yhat.shape[0]).sum()\n",
    "\n",
    "print('MSE: ', mse)\n",
    "print(\"Stop at iteration: \", iter_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  19.59057165757282\n",
      "Stop at iteration:  1964\n"
     ]
    }
   ],
   "source": [
    "# Stochastic  # better with decaying learning rate -> decaying alpha, so that the loss doesnt jump \n",
    "#prepare the data\n",
    "\n",
    "#X_train,X_test have intercepts\n",
    "\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "w = np.zeros(X_train.shape[1])\n",
    "# print(w.shape)\n",
    "\n",
    "alpha = 0.00001\n",
    "max_iter = 2000  #==> typical to call its epochs\n",
    "loss_old = 10000\n",
    "tol = 0.000001\n",
    "iter_stop = 0\n",
    "# idx = np.arange(0,X_train.shape[0],1)\n",
    "# X_train.shape\n",
    "# np.random.shuffle(idx)\n",
    "loss = 0\n",
    "\n",
    "#define your for loop\n",
    "for i in range(max_iter):\n",
    "    idx = np.arange(0,X_train.shape[0],1)\n",
    "    np.random.shuffle(idx)\n",
    "    for j in idx:\n",
    "        yhat = X_train[j,:].reshape(1,-1)@w\n",
    "        error = yhat - y_train[j]\n",
    "        loss = ((yhat - y_train[j])**2)#.sum()\n",
    "        loss += loss\n",
    "        loss_new = loss/y_train.shape[0]\n",
    "        if np.abs (loss_new-loss_old)<tol: #np.allclose\n",
    "            iter_stop = i\n",
    "            break\n",
    "        loss_old = loss_new\n",
    "        grad = X_train[j,:]*error\n",
    "        w = w-alpha*grad \n",
    "\n",
    "\n",
    "yhat=X_test@w \n",
    "mse = ((yhat - y_test)**2/yhat.shape[0]).sum()\n",
    "# mse = (yhat - y_test)**2\n",
    "\n",
    "print('MSE: ', mse)\n",
    "print(\"Stop at iteration: \", iter_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_yhat(X,w):\n",
    "#     return X@w\n",
    "\n",
    "# def get_mse(yhat,y):\n",
    "#     return ((yhat - y_train)**2/yhat.shape[0]).sum()\n",
    "\n",
    "# def delta_loss(new,old,tol):\n",
    "#     return np.abs (loss_new-loss_old)<tol\n",
    "    \n",
    "# def gradient(X,error):\n",
    "#     return X.T@error\n",
    "\n",
    "# max_epochs = 1000\n",
    "\n",
    "# def h_theta_x (X,w):\n",
    "    \n",
    "#     ''' \n",
    "#     Xshape(m,n)\n",
    "#     wshape(n, )\n",
    "#     return(m, )\n",
    "#     '''\n",
    "#     return X@w\n",
    "\n",
    "# for epoch in range (max_epochs): # epochs max_iters\n",
    "#     idx = np.arange(0,X_train.shape[0],1)\n",
    "#     np.random.shuffle(idx)\n",
    "    \n",
    "#     for i in idx:\n",
    "# #     1. yhat = X_i @w\n",
    "#         X_i = X_train[i,:]\n",
    "# #     prediction\n",
    "# #     yhat () = (1,n) @ (n, )\n",
    "#         yhat = getyhat(X_i,w)\n",
    "    \n",
    "# #     2.error = yhat - y_i\n",
    "# #     y_i (1, )\n",
    "#         y_i = y_train[i]\n",
    "# #     error = (1, )\n",
    "#         error = yhat - y_i\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
