{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "A single decision tree does not perform well as it tends to overfit.  A possible solution is the construct multiple trees to reduce variances.  To make sure each tree is not exactly learning the same thing since it will then be all same trees, we need to inject some differences to these trees (i.e., make them as diverse as possible but at the same time they also see some overlappinp samples).  One simple idea is that each of the tree is trained on a subset of **bootstrapping sample** and then perform some sort of aggregation of the decision.\n",
    "\n",
    "The process has the following steps:\n",
    "\n",
    "1. Sample $m$ times **with replacement** from the original training data\n",
    "2. Repeat $B$ times to generate $B$ \"boostrapped\" training datasets $D_1, D_2, \\cdots, D_B$\n",
    "3. Train $B$ trees using the training datasets $D_1, D_2, \\cdots, D_B$ \n",
    "\n",
    "Boostrapping the data plus performing some sort of aggregation (averaging or majority votes) is called **boostrap aggregation** or **bagging**.\n",
    "\n",
    "*Example*:\n",
    "\n",
    "Assume that we have a training set where $m=4$, and $n=2$:\n",
    "\n",
    "$$D = {(x_1, y_1), (x_2, y_2), (x_3, y_3), (x_4, y_4)}$$\n",
    "\n",
    "We generate, say, $B = 3$ datasets by boostrapping:\n",
    "\n",
    "$$D_1 = {(x_1, y_1), (x_2, y_2), (x_3, y_3), (x_3, y_3)}$$\n",
    "$$D_2 = {(x_1, y_1), (x_4, y_4), (x_4, y_4), (x_3, y_3)}$$\n",
    "$$D_3 = {(x_1, y_1), (x_1, y_1), (x_2, y_2), (x_2, y_2)}$$\n",
    "\n",
    "We can then train 3 trees.\n",
    "\n",
    "Note: When sampling is performed **without** replacement, it is called **pasting**.  In other words, both bagging and pasting allow training instacnes to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor.\n",
    "\n",
    "Let's try to code from scratch.  To make our life easier, we shall use DecisionTree from the sklearn library (since we already code it from scratch in the previous class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                test_size=0.3, shuffle=True, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_size 84\n",
      "Tree 0 0.9777777777777777\n",
      "Tree 1 0.9387755102040817\n",
      "Tree 2 0.9166666666666666\n",
      "Tree 3 0.8823529411764706\n",
      "Tree 4 0.9423076923076923\n",
      "===== Average out of bag score =====\n",
      "0.9315761176265378\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import random\n",
    "from scipy import stats\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "\n",
    "class Bagging:\n",
    "    def __init__(self,B,bootstrap_ratio,with_no_replacement = True):\n",
    "        self.B = B\n",
    "        self.bootstrap_ratio = bootstrap_ratio #100% replacement\n",
    "        self.with_no_replacement = with_no_replacement\n",
    "        self.tree_params = {'max_depth': 2, 'criterion':'gini', 'min_samples_split': 5}\n",
    "        self.models = [DecisionTreeClassifier(**self.tree_params) for _ in range(B)] # 5 different decision trees\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        m, n = X.shape\n",
    "        # print(m,n)\n",
    "\n",
    "        #sample size for each tree\n",
    "        sample_size = int(self.bootstrap_ratio * len(X))\n",
    "        print('sample_size',sample_size)\n",
    "\n",
    "        xsamples = np.zeros((self.B, sample_size, n)) \n",
    "        ysamples = np.zeros((self.B, sample_size))\n",
    "        '''\n",
    "        xsamples = (#trees, sample_size, features)\n",
    "        ysamples = (#trees, sample_size)\n",
    "        '''\n",
    "        \n",
    "        xsamples_oob = [] # list because length is not known\n",
    "        ysamples_oob = []\n",
    "        # print(ysamples.shape)\n",
    "\n",
    "#subsamples for each model\n",
    "        for i in range(self.B):\n",
    "            ##sampling with replacement; i.e., sample can occur more than once\n",
    "            # for the same predictor\n",
    "            oob_idx = []\n",
    "            idxes = []\n",
    "            for j in range(sample_size):\n",
    "                idx = random.randrange(m)   #<----with replacement #change so no repetition\n",
    "                if self.with_no_replacement:\n",
    "                    while idx in idxes:\n",
    "                        idx = random.randrange(m) \n",
    "                # print(idx)\n",
    "                # print('X_train',X_train[idx])\n",
    "                idxes.append(idx)\n",
    "                oob_idx.append(idx)\n",
    "                xsamples[i, j, :] = X[idx]\n",
    "                # print('xsamples',xsamples)\n",
    "                ysamples[i, j] = y[idx]\n",
    "            mask = np.zeros((m),dtype = bool)\n",
    "            mask[oob_idx] = True\n",
    "            xsamples_oob.append(X[~mask])\n",
    "            ysamples_oob.append(y[~mask])\n",
    "\n",
    "                # print('xsamples',xsamples.shape)\n",
    "                # print('ysamples',ysamples.shape)\n",
    "                # print('xsamples',xsamples[2,:])\n",
    "\n",
    "            #fitting each estimator\n",
    "        oob_score = 0\n",
    "        for i, model in enumerate(self.models):\n",
    "#             print(i)\n",
    "            _X = xsamples[i, :]\n",
    "#             print('_X',_X.shape)\n",
    "            _y = ysamples[i, :]\n",
    "#             print('_y',_y.shape)\n",
    "            model.fit(_X, _y)\n",
    "\n",
    "            #calculating oob score\n",
    "            _X_test = np.asarray(xsamples_oob[i])\n",
    "            _y_test = np.asarray(ysamples_oob[i])\n",
    "            yhat = model.predict(_X_test)\n",
    "            oob_score += accuracy_score(_y_test,yhat) # acc = _y_test,yhat #oob_score: sum of acc of all models\n",
    "        #     print('oob_score',oob_score)\n",
    "            print(f\"Tree {i}\",accuracy_score(_y_test,yhat))\n",
    "        self.avg_oob_score = oob_score/len(self.models)\n",
    "        print(\"===== Average out of bag score =====\")\n",
    "        print(self.avg_oob_score)\n",
    "\n",
    "#make prediction and return the probabilities\n",
    "    def predict(self,X): #<== X_test\n",
    "        predictions = np.zeros((self.B, X.shape[0])) #X_test.shape[0]\n",
    "        for i, model in enumerate(self.models):\n",
    "            yhat = model.predict(X)\n",
    "        #     print(yhat)\n",
    "            predictions[i, :] = yhat\n",
    "#         print(predictions)\n",
    "#         print(stats.mode(predictions)[0])\n",
    "        return stats.mode(predictions)[0][0]\n",
    "    \n",
    "# print(stats.mode(prediction))\n",
    "# print(xsamples.shape)\n",
    "# print(xsamples_oob)\n",
    "# print(ysamples_oob)\n",
    "# print(mask)\n",
    "# print(idxes)\n",
    "# print(oob_idx)\n",
    "\n",
    "model = Bagging(B=5, bootstrap_ratio =0.8,with_no_replacement = False )\n",
    "model.fit(X_train,y_train)\n",
    "yhat = model.predict(X_test)\n",
    "# print(yhat)\n",
    "print(classification_report(y_test, yhat))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "'''\n",
    "To perform in sklearn, we can use the BaggingClassifier API.  \n",
    "Pasting can be done using BaggingClassifier< setting boostrap=False\n",
    "'''\n",
    "\n",
    "bag = BaggingClassifier(tree, n_estimators=5, max_samples=0.99)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "yhat = bag.predict(X_test)\n",
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===Classwork===\n",
    "\n",
    "#### Out of Bag Evaluation\n",
    "\n",
    "Well, it seems like our bagging technique is quite good.  Anyhow, one interesting observation is that each tree only see a subset of the dataset. Any data that a particular tree did not see is called **out of bag** (oob).  Note that oob is not the same for all predictors.\n",
    "\n",
    "One interesting thing is that since oob is something that each tree never see, thus oob is somewhat a validation set.  Thus what we can do is after we fit each tree. We can ask each tree to test their accuracy with their own oob, and then we can average the accuracy from all trees.  \n",
    "\n",
    "<strong>Your work: Let's modify the above scratch code to</strong>\n",
    "    <ol>\n",
    "        <li>Calculate for oob evaluation for each bootstrapped dataset, and also the average score</li>\n",
    "        <li>Change the code to \"without replacement\"\n",
    "        <li>Put everything into a class <code>Bagging</code>.  It should have at least two methods, <code>fit(X_train, y_train)</code>, and <code>predict(X_test)</code></li>\n",
    "    </ol>\n",
    "No score, no pressure, only intrinic motivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
